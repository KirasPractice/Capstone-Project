{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_TmqtP6Mi0l",
        "outputId": "5eac292b-dec7-4cb1-f1dd-7704dc9b6ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n"
      ],
      "metadata": {
        "id": "-DzzI7zkMkgb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)  # for consistent results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4bqJNFyMmiv",
        "outputId": "0ff05aa8-dbdc-4d02-972d-6dd0221afc9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Once upon a time, there was a robot whoâ€¦\",\n",
        "    \"Explain photosynthesis to a 10-year-old.\",\n",
        "    \"Write a haiku about the ocean.\",\n",
        "    \"What are the benefits of drinking water?\",\n",
        "    \"Continue this story: The astronaut looked out the window and saw...\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(prompts, start=1):\n",
        "    print(f\"\\nðŸŸ¢ Prompt {i}: {prompt}\")\n",
        "    result = generator(prompt, max_new_tokens=100, num_return_sequences=1)[0]['generated_text']\n",
        "    print(\"ðŸ’¬ GPT Response:\\n\", result)\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcAUHoHwMr57",
        "outputId": "16b6e557-df75-4742-f70c-86b01735ff0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŸ¢ Prompt 1: Once upon a time, there was a robot whoâ€¦\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " Once upon a time, there was a robot whoâ€¦\n",
            "============================================================\n",
            "\n",
            "ðŸŸ¢ Prompt 2: Explain photosynthesis to a 10-year-old.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " Explain photosynthesis to a 10-year-old.\n",
            "\n",
            "This is the beginning of a new approach to understanding the origin of life -- and to discovering the mechanisms that drive it.\n",
            "\n",
            "A new tool:\n",
            "\n",
            "The researchers collected samples of a plant from the field in Italy while they analyzed plants with nitrogen and phosphorus.\n",
            "\n",
            "They then analyzed the plants in a laboratory and compared them with the photosynthetic plants they've found. They found that they're not just different from the plants, but also produce different amounts of the same amino acids\n",
            "============================================================\n",
            "\n",
            "ðŸŸ¢ Prompt 3: Write a haiku about the ocean.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " Write a haiku about the ocean.\n",
            "\n",
            "Pete Dorman's poem is the first modern work to have a haiku about the ocean. In the poem, a man named \"Hans\" (also known as Hens) has a small boat with a sail and a sailboard. It sails around the world doing research and research. His destination is the ocean.\n",
            "\n",
            "Hans tries to solve his problem by asking people to write a haiku about the ocean.\n",
            "\n",
            "The ocean is the most wonderful and precious thing\n",
            "============================================================\n",
            "\n",
            "ðŸŸ¢ Prompt 4: What are the benefits of drinking water?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " What are the benefits of drinking water?\n",
            "\n",
            "The benefits have been described by some as the following:\n",
            "\n",
            "Higher water quality and lower mercury levels.\n",
            "\n",
            "Better overall health.\n",
            "\n",
            "High blood pressure and cholesterol levels.\n",
            "\n",
            "Better mental and physical health and reduced risk for dementia.\n",
            "\n",
            "Increased physical activity.\n",
            "\n",
            "Increased well-being.\n",
            "\n",
            "Better health.\n",
            "\n",
            "It's also worth noting that drinking water is a common source of potassium in the United States, so there's no evidence that drinking water is harmful\n",
            "============================================================\n",
            "\n",
            "ðŸŸ¢ Prompt 5: Continue this story: The astronaut looked out the window and saw...\n",
            "ðŸ’¬ GPT Response:\n",
            " Continue this story: The astronaut looked out the window and saw...\n",
            "\n",
            "A young woman, wearing a blue dress and an orange skirt, stood in line at the airport when she heard a loud bang. The girl was in her early 20s, and she had been traveling with her husband, a retired air force pilot. The man was wearing a suit and tie. He was carrying nothing but a suit and tie. He opened the door and saw the woman. The man didn't answer the door. He was wearing a red T-shirt and a blue vest\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Enter your prompt: \")\n",
        "temp = float(input(\"Enter temperature (0.0â€“1.0): \"))\n",
        "max_len = int(input(\"Enter max tokens: \"))\n",
        "\n",
        "# Ensure temperature is within the valid range\n",
        "if not (0.0 <= temp <= 1.0):\n",
        "    print(\"Warning: Temperature should be between 0.0 and 1.0. Using default temperature (0.7).\")\n",
        "    temp = 0.7\n",
        "\n",
        "# Use the generator object from the transformers library\n",
        "response = generator(prompt, max_new_tokens=max_len, temperature=temp, num_return_sequences=1)[0]['generated_text']\n",
        "print(\"ðŸ’¬ GPT Response:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb6hUdq3NaHi",
        "outputId": "2e44aaef-5367-41d8-f5e5-c0f6abcc5773"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: explain the doctor sues book green eggs and ham to me\n",
            "Enter temperature (0.0â€“1.0): .5\n",
            "Enter max tokens: 150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " explain the doctor sues book green eggs and ham to me for $3.50.\n",
            "\n",
            "The book has a lot of good information on the subject. The book is very good, but I'm not sure it covers all the important stuff. I'm not sure if there are any references, but I'm not sure I'm sure there are any questions or anything.\n",
            "\n",
            "I'll be honest with you, I don't know the answer to any of these questions. I'm not sure if any of these questions are correct. I know that this is a very big deal for me and that I can't take the money. I'm not sure if I'll ever get the money.\n",
            "\n",
            "I'll have to ask you if you want to pay for the book. I can't tell you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Enter your prompt: \")\n",
        "temp = float(input(\"Enter temperature (0.0â€“1.0): \"))\n",
        "max_len = int(input(\"Enter max tokens: \"))\n",
        "\n",
        "# Ensure temperature is within the valid range\n",
        "if not (0.0 <= temp <= 1.0):\n",
        "    print(\"Warning: Temperature should be between 0.0 and 1.0. Using default temperature (0.7).\")\n",
        "    temp = 0.7\n",
        "\n",
        "# Use the generator object from the transformers library\n",
        "response = generator(prompt, max_new_tokens=max_len, temperature=temp, num_return_sequences=1)[0]['generated_text']\n",
        "print(\"ðŸ’¬ GPT Response:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0eMZJU6RfRT",
        "outputId": "9a3ee601-7d4a-49ad-b4c8-065d8e23d440"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: whats the definition of the word head\n",
            "Enter temperature (0.0â€“1.0): .4\n",
            "Enter max tokens: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " whats the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n",
            "\n",
            "What is the definition of the word head?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "temp = float(input(\"Enter temperature (0.0â€“1.0): \"))\n",
        "max_len = int(input(\"Enter max tokens: \"))\n",
        "\n",
        "# Ensure temperature is within the valid range\n",
        "if not (0.0 <= temp <= 1.0):\n",
        "    print(\"Warning: Temperature should be between 0.0 and 1.0. Using default temperature (0.7).\")\n",
        "    temp = 0.7\n",
        "\n",
        "# Use the generator object from the transformers library\n",
        "response = generator(prompt, max_new_tokens=max_len, temperature=temp, num_return_sequences=1)[0]['generated_text']\n",
        "print(\"ðŸ’¬ GPT Response:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS-zbnDcNbHX",
        "outputId": "53f57aa5-36a3-46a5-e94f-ada183d73f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: summarize htis text \"Artificial intelligence is a field of computer science that focuses on creating machines capable of mimicking human intelligence. These systems can learn from data, recognize patterns, make decisions, and even understand natural language. AI is used in many everyday applications, such as voice assistants, recommendation systems, and autonomous vehicles\"\n",
            "Enter temperature (0.0â€“1.0): .5\n",
            "Enter max tokens: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " summarize htis text \"Artificial intelligence is a field of computer science that focuses on creating machines capable of mimicking human intelligence. These systems can learn from data, recognize patterns, make decisions, and even understand natural language. AI is used in many everyday applications, such as voice assistants, recommendation systems, and autonomous vehicles\" https://www.youtube.com/watch?v=3U8Z-QmS6uU&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=1&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=2&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=3&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=4&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=5&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=6&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=7&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=8&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=9&list=PLvDw-QJjCQ-qP-VpJr-gXyz9X-RvQ&index=10&list=PLvDw-QJjCQ-qP-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "temp = float(input(\"Enter temperature (0.0â€“1.0): \"))\n",
        "max_len = int(input(\"Enter max tokens: \"))\n",
        "\n",
        "# Ensure temperature is within the valid range\n",
        "if not (0.0 <= temp <= 1.0):\n",
        "    print(\"Warning: Temperature should be between 0.0 and 1.0. Using default temperature (0.7).\")\n",
        "    temp = 0.7\n",
        "\n",
        "# Use the generator object from the transformers library\n",
        "response = generator(prompt, max_new_tokens=max_len, temperature=temp, num_return_sequences=1)[0]['generated_text']\n",
        "print(\"ðŸ’¬ GPT Response:\\n\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C53iI909R8_c",
        "outputId": "467a3659-caa8-4f95-ce5b-df23e5a445d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: tell me a funny story about cats\n",
            "Enter temperature (0.0â€“1.0): .8\n",
            "Enter max tokens: 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " tell me a funny story about cats that I heard in the woods. The whole family took me to see a young male. It was his first real encounter with someone that I knew. He was really, really sweet. I had just arrived in my trailer a week before and he had been standing in the driveway with his wife and three young dogs, all of them dogs. They were waiting to be found, and I said \"What is it, cat?\" And to them, it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "temp = float(input(\"Enter temperature (0.0â€“1.0): \"))\n",
        "max_len = int(input(\"Enter max tokens: \"))\n",
        "\n",
        "# Ensure temperature is within the valid range\n",
        "if not (0.0 <= temp <= 1.0):\n",
        "    print(\"Warning: Temperature should be between 0.0 and 1.0. Using default temperature (0.7).\")\n",
        "    temp = 0.7\n",
        "\n",
        "# Use the generator object from the transformers library\n",
        "response = generator(prompt, max_new_tokens=max_len, temperature=temp, num_return_sequences=1)[0]['generated_text']\n",
        "print(\"ðŸ’¬ GPT Response:\\n\", response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-urfvPgSmcI",
        "outputId": "04c4283f-81df-410c-f4c2-acb6c0ed13d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: Explain how earth was made\n",
            "Enter temperature (0.0â€“1.0): .5\n",
            "Enter max tokens: 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ’¬ GPT Response:\n",
            " Explain how earth was made from a single, single, single substance, and how it would have evolved in such a way that it would have been the same as all the other things in the universe.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance.\n",
            "\n",
            "And this is what happens when you consider that the universe was created in a single, single, single substance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "buTyY79ITJCe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}